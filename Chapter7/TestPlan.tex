\section{Test Plan}

Although the usability of the software is important, as outlined in section \ref{UI Theory}. Due to the scale of testing that would be needed, and recruitment issues outlined in \cite{ben2015effects}, it was decided that user testing would not be carried out. This was down to an an amalgamation of constraints including time and resources. The testing was also due to take place during the first quarter of 2020, however due to the Covid-19 outbreak it was not possible for this testing to take place. As a result the focus was directed towards better "proving" that the formula was able to identify and classify attack traffic. 


As Nielson's 10 rules for design were consulted during the design stage of the user interface, this will be a good offset to critically evaluate the state of the User Interface (UI). Although this may not fully test the hypothesis 'a simple user interface would improve detection accuracy for all users'; this will highlight whether or not the UI has been created with clarity and simplicity in mind. It would be appropriate to suggest that this hypothesis could be further tested in the future through intensive user trials. In order to conduct these trials, intensive user interface testing would involve the recruiting of two groups of users, both 'experienced' and 'novice'. It is proposed that each of these groups would be comprised of a fairly large number. At this time, due to the lack of resources, incentives, and legal issues that arose through the Viral outbreak this would be impossible.

In order to test the hypothesis, the following methodology shall be used to implement a testing approach. A collection of three log files shall be randomly selected from different websites held by a specific server provider. The author of this thesis has managed to gain access to this data in order to test the software at this developmental stage. The log files will contain all IP GET requests that have made contact with these websites within a period of one month. Each of these individual IP traffic instances shall be run through the formula, before being displayed in a graph. The plan of action is to use the log file data to indicate any potential malicious traffic. Further to this, it is the intention to critically assess some of the high ranking IP instances individually, in order to analyse the architecture of each traffic instance. It is hoped that by doing this the hypothesis that 'there is sufficient data within the log files in order to diagnose with a degree of certainty, potentially malicious IP traffic patterns' shall be consolidated with an answer. 

An alternative method of testing was considered, this was to take 4 random IP instances and run them through the formula for critical evaluation. However, this methodology was set aside as choosing 4 random IPs may have provided trends or flags with very similar characteristics. If the 4 IP instances were chosen by picking IPs with different trends or flags, this may have lead to criticism of bias and may have invalidated the research. By assessing a larger data pool it is easier to highlight any instances of outlying data and whether or not the system can classify malicious traffic.

The way in which the hypothesis is tested by collecting log files, will be an efficient way of collecting data in terms of space and computational resource; this shall be done by assessing whether it was possible to detect any malicious traffic using the log file data. If this was possible, then it may be concluded that the spartan amount of data collected would in turn have a dramatic effect on reducing computational resources. It should also be noted that to do alternate full testing regarding CPU depletion, would involve monitoring all CPU processing on the server of a website for one month. This is both impractical and intrusive. It should also be noted that a website tends to run email and other services, thus, it would be problematic to identify the depletion that accounts for the log file data collection.

In order to test the hypothesis that 'by using a human moderator in combination with a formulaic detection system, more malicious traffic will be correctly identified and less genuine traffic will be misidentified as malicious traffic' the following proposal is made. A test could be performed that finds a semi anomalous reading in the log files with malicious 'trends'. If this particular instance is flagged by the software as being a high risk candidate, but is not a malicious entity the hypothesis has been proven. This is simply due to the fact that the system would have assessed this as a malicious threat, however, after being analysed by a human moderator, the appraisal of context lead to it being descaled as a false alert. It is, therefore believed that this would inherently prove a 'human in the loop' has enhanced accuracy of detection. 

In order to test the ability of the system to detect bots, an IP from the log files that exhibited bot like behaviour was selected and tagged with TEST. In theory this should be picked up by the software in the ANGENT or BOT field within the summary tab inside the UI. This was implemented in order to support the MOSCOW element, proposing that the software MUST 'be able to add or remove a known IP address' for services like google and Bing bots.

A small modification was made to the program in order to enable it to look through all of the IP addresses and work out the risk factor, as well as the number of occurrences. The software then logs this data into a CSV file, where it was displayed in a graphical form, the data can then be interrogated. The results will be discussed in the next section.