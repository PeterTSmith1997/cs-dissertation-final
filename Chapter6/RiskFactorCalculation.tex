\section{Overview of the risk factor}
\subsection{Defining Risk} \label{Define Risk}
The judgement of risk related factors is a difficult concept to define and apply mathematical principals in order to construct a logical value. In day to day life risk is a particularly easy thing to identify. It is easy to define whether an event has aspects of high or low risk. However, it is a much more difficult parameter to breakdown and portray in mathematical terms. Furthermore, this software is proposing a technique that focuses on previously unexplored influences. Therefore logic has been applied to define the weights of risk in many cases due to a lack of prior study and research. As discussed in chapter 4, a lot of risks come from the context of the scenario. Therefore, even though this can be mathematically defined, it is important to accept some limitations with the rational presented within this section. The supposition of logic in many cases has not been implemented with supporting research due to a lack of prior investigation. It is even more important to point out that people cannot agree on the definition of a risk. As \citeauthor{fischhoff1984defining} states 'The focal ingredient in all this has been concern over risk. Yet, the meaning of "risk" has always been fraught with confusion and controversy' (\cite{fischhoff1984defining}).

In order to define risk a formula was proposed in order to look at different vectors that became apparent in the log files. Along with this historical data was applied to compose a mathematical assessment of overall risk rating. It is believed with these two key principals for defining risk, the assessment of incoming traffic can be asserted in terms of its potential for maliciousness.

The body of the risk factor was made up of two distinct partitions. The breakdown of this value is made up from the first partition; 90\% coming from the formula itself. The formula itself will be discussed in section \ref{The Formula}. The second partition is comprised from two smaller sub-partitions and makes up the remaining 10\%; this is comprised from historical data gathered within the database. The two sub-partitions making up the remaining 10\%  have separate weights values. The first is data collected from the last 30 days and given a 75\% weight, the second is data collected from all time and given a weight of 25\%.

The use of the historical data is to try and give the system some idea of context, as it helps the system see if an IP has shown up and been reported in another log file. A paper by \citeauthor{reiss2007efficient} looks at the ever evolving methodology in utilising streamed data for research. \citeauthor{reiss2007efficient} states that: "While current reasoning approaches are designed to work on mainly static data, the Web is, on the other hand, extremely dynamic: information is frequently changed and updated, and new data is continuously generated from a huge number of sources, often at high rate. In other words, fresh information is constantly made available in the form of streams of new data and updates." (\cite{reiss2007efficient}) As assessing streamed data is a relatively new concept, it would be reasonable to assume that more recent data, is more relevant data. Therefore a heavier weighting is given to the last 30 days as this is to try and assess if the IP is currently or actively attacking other websites. In addition to this, IPs sometimes change, this was why after 30 days a lesser weighting is given to the IP. It would be appropriate that this factor still forms part of the calculation. This is due to the fact that an IP may be used in rotation therefore, it may be quiet for a while and then begin attacking websites again. 

Users have to manually report suspicious IP addresses; this was a design decision that was hard to make due to the opposing views of the literature. There is an argument that the IPs should be automatically reported to the system and this would have given the system more data. It all comes back to context, in which case a human is better placed to judge, and do so more accurately than any mathematical generalisation could. A human could appraise a variety of factors that a computer could not compensate for. An example could be presented where a user's personal IP would show up more frequently than any other traffic instance; this would potentially lead to a high risk related tag. This reinforces the case that there was the chance that non malicious IPs could be flagged and reported to the system. 

As previously discussed, 90\% of the weighting is derived from the data contained in the log files; this may be regarded as a particularly heavy weighting. The rational behind this decision was due to the IP activity on a website being assessed independently of other websites. The high weighting of the log file means that an IP, even if not known to the database, can still achieve a very high risk factor, providing user feedback about their log file. In the next section, this thesis will analyse the inner-workings of the formula.
%.Why we use historical data
%.the fact that users have to report the data
%.Why some some elements weight more than others.

%end with intro to formullaz