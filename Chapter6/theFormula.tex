\section{The Formula} \label{The Formula}

As was mentioned previously in this chapter, the formula itself is the key factor in defining risk. It is believed that it shall go a long way in diagnosticating the malignancy of malicious IP traffic, while deescalating false alerts with similar architecture.

The formula is broken down into five distinct categories of information, these five variables were chosen as they each look at a different architectural aspects of a traffic instance. It is believed that a malicious IP instance can be detected and identified by looking at the values of these attributes. The code for the formula can be seen in appendix \ref{code}.

\subsection{Bots}

It is well known that bots are used by numerous online companies in order to index and categorise various web-pages. Although some of these instances of traffic can at first sight appear to be malicious, due to their architecture, they are in fact a legitimate form of web traffic. For this reason a process is required within the formula to disclude known bots from any further processing within the formula and disregard them at the identification process. This process should preferably be carried out at an early stage within the program. For this reason it has been decided to set this risk factor to zero for known bots, as they pose no risk to the website.

The data for bots is gathered from other websites and deposited in a database where an admin can sort them. The formula will access this database to search for known bots. According to \citeauthor{Bots} 17.5\% of web traffic was determined to be legitimate bot traffic, for example: Google crawlers. These bots are vital for the maintenance and navigation of the internet, hence the importance of the software to disclude these IP addresses from potential barring. It is suggested that 20.4\% of all web traffic in 2018 was considered to be malicious (\cite{Bots}), most of which was automated in a bot like format.

Due to the fact that fake Google bots or fake search engine bots exist online (\cite{algiryage2018distinguishing}), this first check could be altered in the future to look at whether a bot instance is legitimate. The decision was made not to implement this as part of the formula during this research. The reason for this decision will be discussed during the final conclusions of the research.

 
\subsection{Number of Requests}
The literature would indicate that the more requests a website receives from a single IP, the greater the likelihood that the source IP is malicious. After consideration, it was concluded that an IP occurrence may appear in the log files a large number of times: this may result in an over-inflation of the risk factor. In order to mitigate the potential for this event the decision was made to implement a natural logarithm in order to normalise the data into a more collective state. This, in turn, will help to generate a risk factor that has less chance of procuring a large anomalous value, hence breaking the formula.

An alternative measurement could be formulated from the log file data. This would be done using the time between each successive instance of the same IP address. If the time between each successive instance of the same IP traffic is very small, these types of attack are better mitigated in real time. Cloudflare states that 'ability to implement page rules and populate those changes across the entire network is a critical feature in keeping a site online during an attack'. There is a possible situation that could occur, where web traffic from an IP may come at a staggered rate, however, this may also have the same affect, and the same potential for malignancy. 

After careful consideration it was decided that it would be better to test more attacks by looking at the average time between IP instances; this was generated using the following formula: \[\lnb{\dfrac{Occurrences of IP }{43800 \times No. of log files}}\] It must be noted that the minimum value of this is 0.1 in order to prevent errors in the calculation. 

It would be appropriate to include an explanation of some values within this formula. The figure 43800 is the average number of minutes in one month, based upon 30.4 days being the average number of days within a month. The number of log files is a dynamic value based on the number of files that have been uploaded to the software. This allows multiple months worth of data to be analysed in one sitting.

\subsection{Response}


The response of the server is a good indicator of the legitimacy of a request, the server assigns a http code to the response. The decision was made to break down each request/response and apply an applicable risk level for each response code. The justifications for the applicable risk factors shall be discussed within this subsection. It should be noted that none of the literature reviews used this as an indicator for attack identification. As this is a path finding approach, the application of a risk factor for this element may be more likely to be adjusted through critical review after testing (\cite{ErrorCodes}).

A \textbf{400} error implies that the IP traffic instance made a BAD REQUEST. This would suggest that the user did not come to the site on a natural path from browsing. It would suggest that an error in logic was established during the request. On some web-servers a 400 error message can be broken down into more detail to pinpoint the architectural issue within the request. It should be noted however that the web-servers used for testing with this software do not use  IIS 7.0, IIS 7.5, or IIS 8.0, and as such will not break down full details of these 400 errors.

An invalid timeout response could be indicative of a situation where the GET request packet is invalid or has poor flagging standards. This could be a sign of a low rate bandwidth attack, hence it would be appropriate to apply a risk factor association for the greater 400 response error. 

An invalid destination header might suggest an incorrect http pathing request was inputted and may be indicative of a BOT attempting to formulate potential page targeting unsuccessfully. It may also indicate a handcrafted packet that has been constructed incorrectly or synthetically. As discussed in the literature review signs of a synthetic packet are highly indicative of a malicious request.

Due to the varied vectors that can produce a 400 request and the commonality of this error, it would be fair to apply a risk factor that does not overly penalise the event. After consideration, a lower weight was applied to this error response with the ideology that it may stack after repeated instances of the same error.

A \textbf{401} error would indicate that the user is not authenticated to view the web-page. This could be due to an error in the server settings, which may lead to further unauthorised probes to evaluate weaknesses in the server configuration. This could also be an error raised from the occurrence of too many consecutive requests from the same IP or that the maximum number of allowed requests has been met. This would indicate that perhaps a server owner has set a limit on the amount of IP requests from the same source. There may however, be legitimate reasons for a large number of requests from the same IP within a set time frame. A 401 may also occur if the website runs an API service, the API may generate a 401 error due to invalid credentials. These error messages should be given a suitably high risk factor rating as they are attempts to access forbidden pages or instances of denial. 

A \textbf{403} error is similar to a 401 in that a visitor has passed the authentication stage for reaching a page, however, for some reason they have failed to be permitted access to the page; this could be due to a read, write or execute violation. It could be an error generated from the denial of an IP even if an IP has been rejected due to historical data filtering, this would still generate a 403 error message. Therefore, this would suggest that this specific instance of traffic is most probably an attack, and also most probably automated. This would be fortified by the conclusion that, if a genuine visitor received a 403 message, they would eventually stop sending requests. Alternatively, an automated bot attacker would send a string of requests and continue to receive 403 messages. After consideration it was decided to give this error message a fairly high risk related flag.


A \textbf{404} error is an indicator that the page requested has not been found and is perhaps the most common error response encountered during response protocol. This potentially could mean that a visitor may be searching for a page that does not exist. Although this could be accidental or due to a change in directory navigation. If the error is repeated then this could be a sign of malicious activity. Due to the potential of an accidental or genuine 404 error, this was given a nominal risk factor for association.

A \textbf{429} error flags a scenario where a visitor has made too many requests within a set period of time; this could be indicative of a flood attack or a Denial of service attack. These appear to have a distinct overlapping with some of the criteria for a 401 error and therefore a similar weighting methodology should be applied. 


A \textbf{500} error tends to indicate where the IP is requesting data from. This could be an internal error such as the server is too busy. This may in fact be an indication that the server is under attack from a DoS attack, but does not suggest that this particular IP instance is the culprit. On the other hand, this could also be due to a rewriting or data storage error on the target servers. It could be argued that this is a negligible risk factor however, a repeating IP receiving this response could accumulate a fairly high risk score. This visiting IP would begin to appear as the main culprit of a DNS flood attack through multiplication of this smaller risk factor.

A \textbf{200} response suggests that the visitor request has been successful; this is a potential indicator that a request is legitimate, however, in the case of some discreetly synthesised attack packets, this is not always the case. For this reason a small risk reduction should be applied in order to mitigate risk factors from other variables. The response risk is a signed integer, therefore when you make a large number of requests that return a 200 status the risk factor will be significantly reduced.



\subsection{Pages accessed}

The pages accessed from each visitor could be indicative of a malicious IP traffic status, hence some elements of risk should be appropriately applied. Firstly, if an IP is attempting to request access to WP-admin, which is the standard administrative area of Wordpress websites, it would be appropriate to apply a moderate risk rating. It is common for attackers to attempt brute force attacks at the "WP-admin"; this is typically an attempt to unlock associated admin passwords by flood attempting a guess system in order to bypass security (\cite{Brute}). The attacker, which is normally a bot, will try as many username and password combinations as possible until they find the right one; this makes accounts with weaker password combinations particularly vulnerable, for example 'password123'. It should be noted that there may be genuine reasons that users may need to log into WP-admin. For example, a blogging website may have multiple bloggers, hence the reason numerous individuals would require access to the WP-admin page.

If an IP searches for a page containing the word 'login', it would be appropriate to apply a nominal risk factor. This could, however, be a genuine request, the majority of authentic visitors would tend to navigate to the page by a procedural method. It should be noted that if there is no page containing the word login, they would get a higher risk rating cumulatively, due to receiving subsequent 404 errors, hence, distinguishing a malicious trend in the activity of the IP. The inverse is true if there is a login page, then a 200 response status would be applied to the communique, reducing the risk factor slightly.

Although the formula has variables that appear at first to be separate and independent, by combining the risk related data enables the building of a 'larger picture' of the trends associated with a visiting IP.  An example of this would be the status and the requested URL. Through correlating variables, malicious access attempts are highlighted and less aggressive traffic is mitigated accordingly.

The response size should be taken into consideration as an element of potential malignancy. If a request generates a server response with a size of zero, this may signify that the connection was closed before a response was returned. An event such as this could be indicative of a bot attack, or perhaps a GET request containing inappropriate flags.

\subsection{Country}

After consideration of data made available online, it appears that the country of origin for IP traffic can potentially infer a greater risk. After intensive research during the writing of this thesis, it was discovered that the USA was the origin of 45\% of all malicious traffic (\cite{Webattacks}). It should also be noted that the USA has the third highest population of active internet users in the world. Interestingly, this factor does not diminish the phenomenally high percentage of malicious traffic coming from that country. It therefore would be negligent to dismiss a country risk factor from the origin of IPs. It should be noted that some countries, such as China, have a particularly prohibitive fire-walling for internet access which is enforced through governmental regulations (\cite{China}). The possibility of forcing malicious traffic from countries such as China through a VPN or proxy, in which case the initial country would be disguised from the software is a limitation that can not be underestimated. It is important to note that although the software has the ability to detect the country of origin for the IP source location, it does not have the ability to assess the target websites location. This illustrates how important 'context' is, as for example, a letting agency in the UK would expect a higher rate of UK IP traffic. This highlights the importance of a human decision maker within the detection methodology in order to access the risk related factor in terms of the context of the event (\cite{cranor2008framework}). Due to the increasing ability for users to mask their location online it may be unfair to allocate a high risk factor for this element. Instead it was more appropriate to apply minimal risk factor in order to assess its effects upon the monitoring of potential threats.

To illustrate the risk factor for country of IP source it was decided to rate each country in terms of risk between 1 and 100. These were formulated using the data supplied at the time of research from (\cite{Webattacks}). A percentile approach was used to illustrate risk scoring based on the number of attacks that had originated for the country source from the statistics from this website. There is a UI built into the admin section that will allow an administrator to update the country risk. The logic that is mentioned above is enforced.

\subsection{Conclusion}

After consideration of the numerous risk associated variables, the following risk weightings are proposed and have been documented in appendix \ref{Weights}. As has been shown in section \ref{Define Risk} there are inherent difficulties in defining risk using a mathematical approach, however, by utilising the information documented in the section above it is imagined that the following mathematical methodology can be used to define the risk of web traffic instances. 

\[risk = (orrcancesOfipLog \times 0.6) + ((requestRisk+responseRisk) \times 0.3) + (countryRisk \times  0.1) \]

It is believed that this formula would in part go some way to replicate the intuitive process that a human moderator would go to in order to diagnosticate malicious traffic. After running data through the formula, the outcome shall be a value between 1 and 100, the final integer is the value shown to the user in order to indicate overall risk. It is important to correctly weigh each variable with the appropriate \%. After careful analysis of the threat vectors it was decided to give the largest weights to the occurrences of IP. The decision was made to do this, as the vast majority of attacks share a correlated feature of large numbers of successive visits to the same site from the same IP. 

The request and response is the second most important factor in the formula; this is the accumulation of the response codes and the request is the page they are attempting to access. These two factors are combined equally to make up the 30\% portion of the overall formula. The decision to apply this weight was due to the multitude of different associated code risk factors. An accumulation of internal response events could have too much impact upon the overall outcome of the formula if the weight was set beyond 30\%. These variables are calculated independently as in future these may require separate weights.

As discussed during this chapter, the country of origin is a particularly difficult weight to define. It has not been used as a key indicator of risk factor in prior methodology, hence, it is a relatively experimental risk flag. It must also be noted that due to the increasing use of VPN activity online, it is becoming more and more unlikely to know for certain the true country of origin for any IP source (\cite{GoGlobe}). Hence the risk factor is at this experimental stage, particularly ineffectual on the outcome of the overall formula.

