\section{Hypothesis}

The overall question behind the research project discussed in this thesis was to establish whether there was a better way of detecting malicious traffic. In essence, is there a way to identify and flag malicious IP requests with a formula based around the GET request characteristics. However, it would be appropriate to portray the purpose of the research with the following set of hypotheses.

Firstly, it is hypothesised that the process of collecting log files will be an efficient way of collecting data in terms of space and computational resource. It was highlighted in the \citeyear{staniford2002practical} paper by \citeauthor{staniford2002practical} that it is considered completely unfeasible to save all network traffic for any sustained amount of time as there would be too much data to store. Hence, collecting information with variables suitable for study in the condensed format would be invaluable for the performance of the software. This was a hypothesis that was promoted by the issues uncovered during the literature review of \cite{Adi2015} and \cite{Adi2016}. These papers suggested that looking at a large data set may be problematic due to the constraints of processor depletion. It is therefore hypothesised that by only collecting log file data, that is collected by default anyway, will mitigate the workload effects that cause CPU redundancy.

It is predicted that there is sufficient data within the log files in order to diagnose, with a degree of certainty, potentially malicious IP traffic patterns. It could do this by looking at the characteristics of each traffic instance. It can therefore be hypothesised that a formula could be developed as an aide for the identification of potentially malicious IP traffic instances by using the characteristics of each traffic instance and portraying them as variables. It is proposed that the overview of a human moderator could look upon the potential threats and assess their elements. They can then evaluate and take appropriate action, after confirming the status of a malicious IP instance.

It would be logical to postulate that a simple user interface would improve detection accuracy for users. This would be enforced by ensuring that website owners can analyse data in a understandable way. During the literature review there was no study that explored a user interface design specifically related to a detection software. Therefore, it could be argued that there is a definitive need for a user friendly UI, and this should be a critical element in the design architecture. Details and approaches for the intended user interface will be discussed later in the Chapter.

It is hypothesised that using a human moderator in combination with a formulaic detection system, more malicious traffic will be correctly identified and less genuine traffic will be misidentified as malicious traffic. The formula's mandate is to identify potential threats, however, it shall not be used to make the final decision on any IP instance in terms of countered action. Many of the thesis read as part of the literature review used a singular approach of either artificial intelligence or a formulaic diagnostic in order to identify malicious traffic. In most cases these approaches admitted to a degree of uncertainty in their methods of identification. For this reason it has been decided to implement some kind of manual assessment of the incoming data overseen by a human moderator. When implementing a fix for this at the design level, the decision was made to use the access logs which are already collected by default on web-servers as already mentioned earlier in this chapter. The human moderator would then be able to access the full details of the traffic instance and investigate its potential malignancy, hence improving the degree of accuracy of identification.  

