
\section{User understanding} \label{UI Theory}
When identifying threats it is not only the computer's ability to model threat vectors, that is of course very important, however, it is also the ability of the user to manually identify the threats and appropriately classify them. Theory would suggest that people with a Cyber security background would be better able to identify such threats than people without such experience. In order to design the software and an appropriate user interface, it is important to look at the human factors involved. It is assumed that humans in general can and will act in an unpredictable manner, due to differences in cognitive abilities. This section will look into the key factors that affect user understanding in order to better comprehend the principles involved in building a suitable user interface. It will also debate the ability of non-Cyber security experts and their capacity for moderating potential threats at an administration level. These are both important factors as they will assess the overall ability for a human moderated system's functionality in terms of accuracy of threat detection.

Crannor stated in 2008 'Many secure systems rely on a “human in the loop” to perform security-critical functions, however, humans often fail in their security roles. Whenever possible, secure system designers should find methods of keeping 'humans out of the loop' (\cite{cranor2008framework}). This should be taken into account when designing the system. Crannor's take on the human element within a security protocol promotes the integration of a feature in the software to automatically block suspicious IP instances. However, it has been identified from prior papers that many AI systems based around formulaic rational can be generalised, and have a degree of uncertainty and error (\cite{kim2008slow}). Due to the conflicting array of arguments within the field regarding whether or not keeping a human within the loop for analysis is beneficial, as will be explored in section \ref{Define Risk}, it would be logical to assume that risk is a difficult concept to define. This would therefore make a total AI build both impractical and potentially a system fraught with uncertainty.

It was suggested within Noam Ben-Asher's 2015 paper that effective design of information security systems that support the work of the human security analyst, may benefit from a better understanding of the capabilities and limitations of the human decision maker who uses them (\cite{ben2015effects}). For this reason it is imperative to design a user interface that promotes understanding of the software during the synthesis. It also fortifies the need for further reading into studies that assess human computer interaction and decision making in general. In Noam Ben-Asher's 2009 publication he stated 'for users to accept security mechanisms, they must be aware of the security threats and understand the importance of security.' (\cite{ben2009experimental})

In his 2015 study Ben-Asher attempts to see if there is a link between cyber security knowledge and the ability to detect attacks, and or malicious network traffic. The experiment comprised of two separate groups, an expert group comprised of 20 individuals, and a novice group comprised of 55 university participants. 

In order to better understand the differences in efficiency between an expert and novice, in terms of their ability to identify incoming malicious traffic, the research undertaken by \citeauthor{ben2015effects} was reviewed. In this research the author set up testing of two distinct categories of individual to assess their performance in identifying malicious network activity. The first group of test subjects had advanced knowledge in the field of cyber security, and the second group were novices' having no prior knowledge of network or cyber security methodology. 

As part of this testing the candidates were examined on their prior knowledge in the areas of theory and understanding on the definitions of attack. In review of this literature this was an excellent way of validating the candidates authenticity for each test group. The candidates were then evaluated in their capacity to identify malicious network activity in a practical manner and given a large pool of network events to assess. Novice participants were given 10 scenarios containing 20 network events in order to determine the maliciousness or 'lack of' in each case. Experts were given a smaller load of 3 scenarios containing 20 network events and were similarly asked to identify threats. These 20 events were chosen at random from a large pool which was reflective of the size of each candidate group (\cite{ben2015effects}). The reason given by the researchers for the reduced expectations of expert participants was a concern over their willingness to participate.

It could be argued that there are three weaknesses with the study conducted by \citeauthor{ben2015effects}. The first issue is that the candidates were given a time limit in order to answer questions; this may have introduced a stress factor to the scenario that was not required for the overall purpose of the research. Research by Keinan in 1987, showed that elevated stress levels incurred during timed conditions lead to more errors (\cite{Keinan}). This would indicate that a system without any time pressure features would be instrumental in reducing errors. In the case of Ben-Asher's study, these stress factors were increased every 10 seconds and would have introduced an unnecessary stress variable. One of the key goals of the software, outlined in this paper, is that website owners can use it themselves. The website owners may not have much, if any, cyber security knowledge, however may have good IT skills. The novices would have been under increased stress as they had no prior experience of assessing cyber security threats, thus, affecting the results of the experiment.

Another criticism that could be made is the fact that the expert group completed the assessment online whereas the novices were tested in a controlled and protected environment. It could be argued that not testing them in the same environmental conditions invalidates the research. A study by \citeauthor{russell2003computer} reported that students who are comfortable writing on computer actually do better on computerized writing exams (\cite{russell2003computer}).This builds upon the criticism of the study in terms of the advantages that the Cyber security experts had in this testing. The overall ecological validity becomes compromised here, as the environments do not mirror a real life setting \cite{bryman_2016}. This suggests the comparative element of these results is not achievable due to external factors that influence each individual differently, which can effect circumstances such as stress.

A third, and potentially the most important criticism that could be made of this paper is the different amounts of material that the candidate types were given to assess. It could be argued that fatigue took place during critical stages of the assessment for the novice group as they were tasked with a work load over 300\% larger than the experts. If the researcher was correct in his assumptions that an increased work load would have deterred the cyber security specialists from participating in the experiment, then the work load of both groups should have been equalised. It can be suggested that the dependant variable of measurement has been compromised by an external factor, tainting the results (\cite{bryman_2016}). In this case, that would be the difference in work load between the professional and non-professional groups, thus compromising the research outcome.

Another point can be the differing incentives offered to the two groups, the novices were given \$10 and the ability to gain more based on performance. In comparison, the cyber security professionals who were given an opportunity to win a \$50 amazon voucher with every correct answer earning them a ticket towards that. Further challenges towards the validity of the study can be questioned; there is another external variable, the motivational drive of the individuals towards the task. If the incentives are not only different in amount, but also provide different motivational drives, what level of fairness is there to those participating (\cite{Incentives}).The computer experts are not guaranteed a payment, thus competition may have driven them to complete the tasks, more so than those of the novice groups, who have a guaranteed payment even if it may be smaller. 
When considering the two groups which took part in Ben Asher's study, there are several major flaws which may affect the results of the research, which suggest that Ben Asher's paper lacks internal validity.  If the study was conducted with equal workloads and in an equal environment, disregarding the likelihood of professionals taking part, then these results may have been identical; this in turn may have rendered the research non-applicable. The study does however suggest that a clear user interface will aid users in detecting attacks and therefore, when designing the software of a project, the clear user interface will be required.

%Due to the different workloads and stress factors in the different sample groups, it becomes apparent when applying \citeauthor{bryman_2016} conclusions on internal validity that some major flaws exist in this research. It has not been proven in definitive terms that the user's cyber security knowledge impacts the ability to identify security threats. The results of Ben-Asher's 2015 study suggests an almost equal result level for both groups, it should be noted that when looking at DoS attacks the non experts outperformed the experts (\cite{ben2015effects}). If the study was conducted with equal workloads and in an equal environment, disregarding the likelihood of professionals taking part, then these results may have been identical; this in turn may have rendered the research non-applicable. The study does however suggest that a clear user interface will aid users in detecting attacks and therefore, when designing the software of a project, a clear user interface will be required. 


%The plethora of external variables in Ben-Ashers work can be said to effect the results of the research. It could therefore, be argued that this paper lacks internal validity, which Bryman (2016) postulates upon "If we suggest that x causes y, can we be sure that x is responsible for the variation for y and not something else" (\citeauthor{bryman_2016} \citeyear{bryman_2016};41). Due to the limitations that have been identified, with the use of different incentives, the lack of experiment environment controls between the groups and the number of tasks between the participants, the comparability can not be said to be directly relevant to only the experience of the participant, but also these external variables.
