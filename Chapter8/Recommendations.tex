\section{Recommendations}


The literature reviewed during this thesis has illustrated that there is a need for greater research around the detection of low-rate bandwidth attacks. Those detection methods proposed in chapter 2 by Adi and Tripathi would seem to indicate that there is a problem with detecting and mitigating low-rate bandwidth attacks, especially in relation to HTTP2 protocols. The strategy proposed by Adi of monitoring server resource utilisation does not scale well into a shared environment. This shows that low rate bandwidth attacks cannot be detected in real time unless the server is offline as a result of the attack, therefore, alternative measures need to be considered. As concluded in this thesis, by looking at the log data of a website over a sustained period of time, attacks can be detected and appropriate action can be taken.

There is a need for greater research  on how low rate bandwidth attacks can be detected on a large scale. If companies that are the victims of such attacks were more open about their methods of mitigating said attacks, this would prove enormously beneficial for the wider internet community. It is also worth noting that greater testing would prove favourable creating both a well functioning piece of software and a usable interface. Whilst websites such as abuseipdb allow people to search whether or not an IP has been used for malicious purposes. It feels necessary that work should be undertaken to assess whether or not such websites harm cyber security, by allowing attackers to see if their IP is being detected and reported.

The current work also shows that high rate bandwidth attacks can be easily mitigated and should not pose a problem to website owners if their security systems are implemented appropriately. This work also shows that due to the scale of high rate bandwidth attacks, these are better mitigated using a network of proxy servers, as discussed in chapter 3. A lot of the solutions for the high rate bandwidth attacks rely on collective data from multiple websites, this is why the methods proposed in this thesis apply the same approach to low bandwidth attacks. This is due to the extensive amount of research into high rate bandwidth attacks, and as such, these attacks pose very little threat to websites. 

Overall during the literature review it was discovered that there was a genuine lack of thorough research into the field. In some respects this was encouraging for this thesis, as it allowed for the gestation of new ideas. It does, however, present a clear and present danger regarding the safety of websites in line with the increasing sophistication of attacks.


\subsection{User understanding}

During the literature review it was stated by \citeauthor{cranor2008framework} that some novice PC users may not be 'all that receptive to learning'. This demonstrates that teaching website owners how to use the software may be a problematic endeavour. It may be considered beneficial to implement a simple online tutorial for novice users in order to aid in the learning process for the use of the software. This could be an ideal way to promote understanding of some of the more advanced features within the user interface. This was considered too time consuming to create for the initial software package, however it would be an excellent prospect for a future project or supplement.

In chapter 3 this work demonstrated that there is a distinct lack of good quality research looking at whether or not users can actually identify an attack without significant training. This thesis has also shown that it is safer to use a hybrid of computers and humans to identify malicious traffic, largely due to the number of errors in classifying attacks within the software. Allowing the computer to do the mathematical calculation to define the risk, while allowing the humans to undertake the decision making process of what action, if any, should be taken.

As the main focus of the user interface display within the software was to show each IP that accessed a specific website. It may well have been beneficial to make this table of headers larger and perhaps include the user agent field for easier identification of bots. It may have also been useful to feature a field on the main page that highlights the risk factor. This was not included due to the amount of time it takes to calculate the risk factor, therefore the calculation would need to be more efficient for processing times. However, this improvement could be implemented as an extension to the software at a later stage. This would involve evaluating the effect that this module would have on processing times, and then fine tuning the code to make it run as efficiently as possible.
