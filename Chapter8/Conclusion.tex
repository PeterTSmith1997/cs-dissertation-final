\section{Conclusions}

The literature reviewed during this thesis has illustrated that there is a need for greater research around the detection of low-rate bandwidth attacks. Those detection methods proposed in chapter 2 by Adi and Tripathi would seem to indicate that there is a problem with detecting and mitigating low-rate bandwidth attacks, especially in relation to HTTP2 protocols. The strategy proposed by Adi of monitoring server resource utilisation does not scale well into a shared environment. This shows that low rate bandwidth attacks cannot be detected in real time unless the server is offline as a result of the attack, therefore, alternative measures need to be considered. As concluded in this thesis, by looking at the log data of a website over a sustained period of time, attacks can be detected and appropriate action can be taken.

The current work also shows that high rate bandwidth attacks can be easily mitigated and should not pose a problem to website owners if their security systems are implemented appropriately. This work also shows that due to the scale of high rate bandwidth attacks, these are better mitigated using a network of proxy servers, as discussed in chapter 3. A lot of the solutions for the high rate bandwidth attacks rely on collective data from multiple websites, this is why the methods proposed in this thesis apply the same approach to low bandwidth attacks. This is due to the extensive amount of research into high rate bandwidth attacks, and as such, these attacks pose very little threat to websites. 

Further research is required into the detection and mitigation of low rate DDoS attacks. During the literature review it was discovered that there was a genuine lack of thorough research into the field. In some respects this was encouraging for this thesis, as it allowed for the gestation of new ideas. It does, however, present a clear and present danger regarding the safety of websites in line with the increasing sophistication of attacks.


\subsection{User understanding}

During the literature review it was stated by \citeauthor{cranor2008framework} that some novice PC users may not be 'all that receptive to learning'. This demonstrates that teaching website owners how to use the software may be a problematic endeavour. It may be considered beneficial to implement a simple online tutorial for novice users in order to aid in the learning process for the use of the software. This could be an ideal way to promote understanding of some of the more advanced features within the user interface. This was considered too time consuming to create for the initial software package, however it would be an excellent prospect for a future project or supplement.

In chapter 3 this work demonstrated that there is a distinct lack of good quality research looking at whether or not users can actually identify an attack without significant training. This thesis has also shown that it is safer to use a hybrid of computers and humans to identify malicious traffic, largely due to the number of errors in classifying attacks within the software. Allowing the computer to do the mathematical calculation to define the risk, while allowing the humans to undertake the decision making process of what action, if any, should be taken.






