\section{Evaluation of Product}

In general the software was able to pick up instances of malign activity that make contact with websites. There were very few false alerts and the majority of genuine network traffic was identified as benign. The few false alerts that made it through the detection system would be assessed and deescalated by the human moderator.

Although some elements of the user interface suffered from computational latency, the data was accessed and presented efficiently from the log files. It was clear to see that the software had the ability to detect malign activity with a good degree of accuracy. During the process of coding it was decided to use clearly labeled variables within the code. This was a particularly intuitive move as it now allows these variables to be tweaked for future patching and testing of the integral formula. 

The way in which the code was implemented was well structured and commented; This has allowed for the prospect of an easy 'pick up' if programmers wish to amend, adjust or improve the software in the future. During the creation of the software the researcher learned that the concise labeling and commenting of code was key; This was done as the researcher progressed through the coding, and was instrumental in saving time when navigating to areas that required amendment. Also, the logical naming of methods and classes helped to navigate around the code.

Due to the fact that fake google bots or fake search engine bots exist online, this first check could be altered in the future to look at whether a bot instance is legitimate. The decision was made not to implement this potential for additional risk as part of the formula during this research. This was due to the fact that filter logging genuine bots as a researcher may have taken away valuable insights into the detection methodology and potential flaws of the software. 

After researching further into IIS web-servers it was discovered that a great deal of further information can be given regarding the specifics of response codes. This is illustrated in section \ref{Errorcodes}. The software was built using using Litespeed, hence the error codes were not expanded upon. In hindsight a better version of the formula could be generated utilizing pinpointed error code information from IIS code returns. This may also have helped to disclude some 'over-banding' principles in the risk factor, an example of this can be seen through the ISS breakdowns of 400 error messages.

Another issue with the analysis of response codes, arose through evaluation of the software. It became apparent that a 403 error may in fact be indicative of an affirmative defensive action taken by a user. A large number of 403 responses may reflect that a website owner had blocked an IP address through firewall options. This may have been carried out after a website owner identified malicious trends in a certain IP's visiting habits. This is a difficult factor to discuss as it is highly dependant upon the knowledge base of the website owner. A potential example of this can be seen with the IP of 35.187.118.51. The request pattern shows up as a high number of IP requests over a set period of time. After delving deeper into the data set, it became apparent that the IP was visiting the website approximately 800 times per day. It is theorised that the website owner has identified this IP address as a potentially malicious visitor. This was due to the fact that after a prolonged period of malicious activity the response coding changed to a long string of 403 error messages until the end of the data set. Of course this is speculative, as there would be no way to prove this theory without contacting the website owner directly. It may have been useful to see what risk factor the IP in question would come up with if it was not already returning 403 errors, hence assessing the change in threat scoring after the website owner presumably flagged the IP as malicious.

Another issue that was identified during the process of software testing was the significant number of threat vectors originating from the USA. The current software did not include the ability to appropriate a risk related factor for possible VPN activity. As has been illustrated, the risk element of the country of origin was a trailblazing exercise in this study. It is believed that a large number of other countries would use the USA as a proxy through the use of VPNs. Although at this time this is a theory based assumption, it is predicted that the USA would be a 'go to' region for a VPN location. This is due to the overall 'liberty' in terms of website accessibility from the USA. For this reason it can be assumed that a separate risk factor should be developed and added to the core formula in order to risk assess VPNs in general.

